{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных, установка зависимостей\n",
        "!wget https://lodmedia.hb.bizmrg.com/case_files/815472/train_dataset_train.zip\n",
        "!wget https://lodmedia.hb.bizmrg.com/case_files/815472/test_dataset_test.zip\n",
        "!wget https://lodmedia.hb.bizmrg.com/cases/815472/AltGU.zip\n",
        "!unzip AltGU.zip\n",
        "!unzip train_dataset_train.zip\n",
        "!unzip test_dataset_test.zip\n",
        "!pip install pymorphy2\n",
        "!pip install googletrans==3.1.0a0\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "KFft-0KWgk7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13DasGS6eo3y"
      },
      "outputs": [],
      "source": [
        "# Ссылка для загрузки данных солнечной активности\n",
        "https://www.sidc.be/silso/INFO/sndtotcsv.php"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import dbscan,DBSCAN\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier, SGDRegressor, RidgeClassifier, LogisticRegressionCV,Ridge,QuantileRegressor,PassiveAggressiveClassifier,PoissonRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier,RandomForestClassifier,VotingClassifier,RandomForestRegressor,GradientBoostingClassifier,GradientBoostingRegressor,StackingRegressor,BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC,LinearSVR,SVR,NuSVR\n",
        "from sklearn.decomposition import TruncatedSVD,PCA,FactorAnalysis,IncrementalPCA,FastICA,KernelPCA,NMF\n",
        "from sklearn.preprocessing import RobustScaler,QuantileTransformer,PowerTransformer,PolynomialFeatures,KBinsDiscretizer,StandardScaler,OneHotEncoder,OrdinalEncoder,FunctionTransformer,MaxAbsScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline,FeatureUnion,TransformerMixin\n",
        "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor,LocalOutlierFactor\n",
        "from sklearn.model_selection import train_test_split,ShuffleSplit,StratifiedShuffleSplit,TimeSeriesSplit,GridSearchCV,KFold\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import KNNImputer,SimpleImputer\n",
        "from sklearn.dummy import DummyRegressor,DummyClassifier\n",
        "from sklearn import set_config\n",
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error,roc_auc_score,accuracy_score,f1_score,classification_report\n",
        "import pymorphy2\n",
        "from googletrans import Translator\n",
        "import catboost"
      ],
      "metadata": {
        "id": "QYiJW5bQgk0d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных\n",
        "employee=pd.read_csv('/content/employees.csv')\n",
        "trainc=pd.read_csv('/content/train_comments.csv')\n",
        "traini=pd.read_csv('/content/train_issues.csv')\n",
        "testc=pd.read_csv('/content/test_comments.csv')\n",
        "testi=pd.read_csv('/content/test_issues.csv')"
      ],
      "metadata": {
        "id": "uBCUI5QlkJr6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Генерация дополнительных переменных"
      ],
      "metadata": {
        "id": "mpS-dfUhzEBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Дополнительные переменные времени\n",
        "\n",
        "traini['created']=pd.to_datetime(traini['created'])\n",
        "traini['dayofweek']=traini['created'].dt.dayofweek\n",
        "traini['dayofyear']=traini['created'].dt.dayofyear\n",
        "traini['year']=traini['created'].dt.year\n",
        "traini['month']=traini['created'].dt.month\n",
        "traini['day']=traini['created'].dt.day\n",
        "traini['hour']=traini['created'].dt.hour\n",
        "\n",
        "testi['created']=pd.to_datetime(testi['created'])\n",
        "testi['dayofweek']=testi['created'].dt.dayofweek\n",
        "testi['dayofyear']=testi['created'].dt.dayofyear\n",
        "testi['year']=testi['created'].dt.year\n",
        "testi['month']=testi['created'].dt.month\n",
        "testi['day']=testi['created'].dt.day\n",
        "testi['hour']=testi['created'].dt.hour\n"
      ],
      "metadata": {
        "id": "saYCBYC2lAru"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Число комментариев\n",
        "ccdict=dict(zip(trainc['issue_id'].value_counts().index,\n",
        "                trainc['issue_id'].value_counts()))\n",
        "ccdict.update(dict(zip(list(set(traini['id'].tolist()).difference(set(trainc['issue_id'].tolist()))),\n",
        "                 len(set(traini['id'].tolist()).difference(set(trainc['issue_id'].tolist())))*[0])))\n",
        "traini['ncom']=list(map(lambda x: ccdict[x],traini['id'].tolist()))\n",
        "ccdictt=dict(zip(testc['issue_id'].value_counts().index,\n",
        "                testc['issue_id'].value_counts()))\n",
        "ccdictt.update(dict(zip(list(set(testi['id'].tolist()).difference(set(testc['issue_id'].tolist()))),\n",
        "                 len(set(testi['id'].tolist()).difference(set(testc['issue_id'].tolist())))*[0])))\n",
        "testi['ncom']=list(map(lambda x: ccdictt[x],testi['id'].tolist()))"
      ],
      "metadata": {
        "id": "VaFYvswUkzlt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Конкатенированные тексты комментариев\n",
        "tcdict=(dict(zip(trainc['issue_id'].value_counts().index.tolist(),\n",
        "                [' '.join(trainc['text'][trainc['issue_id']==i].astype('str').str.replace('\\n',' ').str.replace(r'(\\[.+\\])',' ').tolist()) \n",
        "                for i in trainc['issue_id'].value_counts().index])))\n",
        "tcdict.update(dict(zip(list(set(traini['id'].tolist()).difference(set(trainc['issue_id'].tolist()))),\n",
        "                 len(set(traini['id'].tolist()).difference(set(trainc['issue_id'].tolist())))*[''])))\n",
        "traini['texts']=list(map(lambda x: tcdict[x],traini['id'].tolist()))\n",
        "tcdictt=(dict(zip(testc['issue_id'].value_counts().index.tolist(),\n",
        "                [' '.join(testc['text'][testc['issue_id']==i].astype('str').str.replace('\\n',' ').str.replace(r'(\\[.+\\])',' ').tolist()) \n",
        "                for i in testc['issue_id'].value_counts().index])))\n",
        "tcdictt.update(dict(zip(list(set(testi['id'].tolist()).difference(set(testc['issue_id'].tolist()))),\n",
        "                 len(set(testi['id'].tolist()).difference(set(testc['issue_id'].tolist())))*[''])))\n",
        "testi['texts']=list(map(lambda x: tcdictt[x],testi['id'].tolist()))\n",
        "\n",
        "# Конкатенированные авторы комментариев\n",
        "tadict=(dict(zip(trainc['issue_id'].value_counts().index.tolist(),\n",
        "                [' '.join(trainc['author_id'][trainc['issue_id']==i].astype('str').tolist())\n",
        "                for i in trainc['issue_id'].value_counts().index])))\n",
        "tadict.update(dict(zip(list(set(traini['id'].tolist()).difference(set(trainc['issue_id'].tolist()))),\n",
        "                 len(set(traini['id'].tolist()).difference(set(trainc['issue_id'].tolist())))*[''])))\n",
        "traini['author_ids']=list(map(lambda x: tadict[x],traini['id'].tolist()))\n",
        "tadictt=(dict(zip(testc['issue_id'].value_counts().index.tolist(),\n",
        "                [' '.join(testc['author_id'][testc['issue_id']==i].astype('str').tolist())\n",
        "                for i in testc['issue_id'].value_counts().index])))\n",
        "tadictt.update(dict(zip(list(set(testi['id'].tolist()).difference(set(testc['issue_id'].tolist()))),\n",
        "                 len(set(testi['id'].tolist()).difference(set(testc['issue_id'].tolist())))*[''])))\n",
        "testi['author_ids']=list(map(lambda x: tadictt[x],testi['id'].tolist()))"
      ],
      "metadata": {
        "id": "RMCMinsAlATU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Внесение дополнительной информации о работниках\n",
        "edict=dict(zip(employee['id'],employee[['active', 'full_name', 'position', 'hiring_type', 'payment_type',\n",
        "       'passport', 'is_nda_signed',\n",
        "       'is_labor_contract_signed', 'is_added_to_internal_chats',\n",
        "       'is_added_one_to_one']].values))\n",
        "for cnt1 in ['assignee_id','creator_id']:\n",
        "  for cnt2 in list(enumerate(['active', 'full_name', 'position', 'hiring_type', 'payment_type',\n",
        "       'passport', 'is_nda_signed',\n",
        "       'is_labor_contract_signed', 'is_added_to_internal_chats',\n",
        "       'is_added_one_to_one'])):\n",
        "    traini[cnt1+'_'+cnt2[1]]=np.array(list(map(lambda x: edict[x],traini['assignee_id'].tolist())))[:,cnt2[0]]\n",
        "for cnt1 in ['assignee_id','creator_id']:\n",
        "  for cnt2 in list(enumerate(['active', 'full_name', 'position', 'hiring_type', 'payment_type',\n",
        "       'passport', 'is_nda_signed',\n",
        "       'is_labor_contract_signed', 'is_added_to_internal_chats',\n",
        "       'is_added_one_to_one'])):\n",
        "    testi[cnt1+'_'+cnt2[1]]=np.array(list(map(lambda x: edict[x],testi['assignee_id'].tolist())))[:,cnt2[0]]"
      ],
      "metadata": {
        "id": "wPwchSpAqdN_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Леммтизация\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "def lemmatize(text): \n",
        "    words = text.split()\n",
        "    res = list()\n",
        "    for word in words:\n",
        "        p = morph.parse(word)[0]\n",
        "        res.append(p.normal_form)\n",
        "\n",
        "    return ' '.join(res)\n",
        "traini['summary_l']=list(map(lemmatize, traini['summary'].tolist()))\n",
        "testi['summary_l']=list(map(lemmatize, testi['summary'].tolist()))\n",
        "traini['texts_l']=list(map(lemmatize, traini['texts'].tolist()))\n",
        "testi['texts_l']=list(map(lemmatize, testi['texts'].tolist()))"
      ],
      "metadata": {
        "id": "VD9vvtk1p0_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(service_urls=['translate.googleapis.com'])\n",
        "def langdet (x):\n",
        "  lang=translator.detect(x).lang\n",
        "  if type(lang)==list:\n",
        "    lang=' '.join(lang)\n",
        "  return lang\n",
        "# Детекция языка резюме и комментариев\n",
        "traini['summary_lang'] = list(map(langdet, traini['summary'].astype('str').tolist()))\n",
        "traini['texts_lang'] = list(map(langdet, traini['texts'].astype('str').tolist()))\n",
        "testi['texts_lang'] = list(map( langdet, testi['texts'].astype('str').tolist()))\n",
        "testi['summary_lang'] = list(map( langdet, testi['summary'].astype('str').tolist()))\n",
        "# Перевод на английский языка резюме и комментариев\n",
        "traini['summary_tr']=list(map( lambda x: translator.translate(x,dest='en').text, traini['summary'].astype('str').tolist()))\n",
        "traini['texts_tr']=list(map( lambda x: translator.translate(x,dest='en').text, traini['texts'].astype('str').tolist()))\n",
        "testi['summary_tr']=list(map( lambda x: translator.translate(x,dest='en').text, testi['summary'].astype('str').tolist()))\n",
        "testi['texts_tr']=list(map( lambda x: translator.translate(x,dest='en').text, testi['texts'].astype('str').tolist()))"
      ],
      "metadata": {
        "id": "8naI1h9sqdKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Внесение данных солнечной активности\n",
        "trainis=traini[['year','month','day','created']].copy()\n",
        "trainis['ind']=trainis.index.tolist()\n",
        "trainis.index=traini['created']\n",
        "trainis=trainis.sort_index()\n",
        "trainis.index=trainis['year'].astype('str')+':'+trainis['month'].astype('str')+':'+trainis['day'].astype('str')\n",
        "testis=testi[['year','month','day','created']].copy()\n",
        "testis['ind']=testis.index.tolist()\n",
        "testis.index=testi['created']\n",
        "testis=testis.sort_index()\n",
        "testis.index=testis['year'].astype('str')+':'+testis['month'].astype('str')+':'+testis['day'].astype('str')\n",
        "\n",
        "sunact=pd.read_csv('/content/drive/MyDrive/Russia3/SN_d_tot_V2.0.csv',sep=';')\n",
        "sunact.columns=['year','month','day','_','sunspots','sunspots2','sunspots3','sunspots4']\n",
        "sunacts=sunact[['sunspots','sunspots2','sunspots3']]\n",
        "sunacts.index=sunact['year'].astype('str')+':'+sunact['month'].astype('str')+':'+sunact['day'].astype('str')\n",
        "for cnt in ['sunspots','sunspots2','sunspots3']:\n",
        "  trainis[cnt]=sunacts[cnt]\n",
        "  testis[cnt]=sunacts[cnt]\n",
        "trainis.index=trainis['ind']\n",
        "trainis=trainis.sort_index()\n",
        "testis.index=testis['ind']\n",
        "testis=testis.sort_index()\n",
        "for cnt in ['sunspots3']:\n",
        "  traini[cnt]=trainis[cnt].tolist()\n",
        "for cnt in ['sunspots3']:\n",
        "  testi[cnt]=testis[cnt].tolist()"
      ],
      "metadata": {
        "id": "Vuo50fMIlAKh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Внесение дня и генерация данных линейной регрессии таргета по дню (временной тренд)\n",
        "traini['days']=(traini['created']-traini['created'].min()).dt.days\n",
        "testi['days']=(testi['created']-traini['created'].min()).dt.days\n",
        "\n",
        "input1=Pipeline([('input',ColumnTransformer([\n",
        "              ('text',CountVectorizer(ngram_range = (1,1),max_df=0.9, min_df=2),'summary'),\n",
        "              ('text2',CountVectorizer(ngram_range = (1,1),max_df=0.9, min_df=2),'author_ids'),\n",
        "              ('onehot',OneHotEncoder(handle_unknown='ignore'),['project_id', 'assignee_id',\n",
        "       'creator_id'])\n",
        "               ])),\n",
        "              ('tfidf',TfidfTransformer())])\n",
        "\n",
        "pipe=Pipeline([('input',ColumnTransformer([\n",
        "              ('inp1',input1,['summary','author_ids','project_id', 'assignee_id','creator_id']),\n",
        "              ('scale',StandardScaler(),['days'])])),\n",
        "              ('rgs',LinearSVR(C=10000,loss='epsilon_insensitive',random_state=0,max_iter=10000))])\n",
        "pipe2=Pipeline([('input',ColumnTransformer([\n",
        "              ('inp1',input1,['summary','author_ids','project_id', 'assignee_id','creator_id']),\n",
        "              ('scale',StandardScaler(),['days'])])),\n",
        "              ('rgs',LinearSVR(C=0.1,loss='squared_epsilon_insensitive',random_state=0,max_iter=10000))])\n",
        "\n",
        "traini['time']=pipe.fit(traini[traini['overall_worklogs']<1e6],traini['overall_worklogs'][traini['overall_worklogs']<1e6],\n",
        "                        rgs__sample_weight=np.power(traini['overall_worklogs'][traini['overall_worklogs']<1e6],0.4)/\n",
        "                        np.power(traini['overall_worklogs'][traini['overall_worklogs']<1e6],0.4).mean()).predict(traini)\n",
        "testi['time']=pipe.predict(testi)\n",
        "traini['time_log']=pipe2.fit(traini[traini['overall_worklogs']<1e6],np.log(traini['overall_worklogs'][traini['overall_worklogs']<1e6]),\n",
        "                        rgs__sample_weight=np.power(traini['overall_worklogs'][traini['overall_worklogs']<1e6],0.4)/\n",
        "                        np.power(traini['overall_worklogs'][traini['overall_worklogs']<1e6],0.4).mean()).predict(traini)\n",
        "testi['time_log']=pipe2.predict(testi)"
      ],
      "metadata": {
        "id": "rZRTvZheqC7U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение модели"
      ],
      "metadata": {
        "id": "9lnndw5g2S5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Начальное обучение catboost регрессора"
      ],
      "metadata": {
        "id": "yewaII2durTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_val,y_train,y_val=train_test_split(\n",
        "       pd.concat((traini[['project_id', \n",
        "       'creator_id', 'dayofweek', 'dayofyear', 'year', 'month', 'day',\n",
        "       'hour', 'assignee_id_active', 'assignee_id_full_name', 'assignee_id_position',\n",
        "       'assignee_id_hiring_type', 'assignee_id_payment_type',\n",
        "       'assignee_id_passport', 'assignee_id_is_nda_signed',\n",
        "       'creator_id_full_name', 'creator_id_position', 'creator_id_hiring_type',\n",
        "       'creator_id_payment_type', 'creator_id_passport',\n",
        "       'creator_id_is_nda_signed', \n",
        "       'creator_id_is_added_to_internal_chats', 'ncom', 'sunspots3','days','time', 'time_log']].fillna(-1),\n",
        "       pd.DataFrame(np.hstack((CountVectorizer(min_df=2).fit_transform(traini['summary'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit_transform(traini['summary_l'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 1),min_df=2).fit_transform(traini['summary_tr'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 4),analyzer=\"char\", min_df=2).fit_transform(traini['key'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 4),analyzer=\"char_wb\",min_df=2).fit_transform(traini['assignee_id_full_name'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 4),analyzer=\"char_wb\",min_df=2).fit_transform(traini['creator_id_full_name'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit_transform(traini['texts'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit_transform(traini['texts_l'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 1),min_df=2).fit_transform(traini['texts_tr'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 1),token_pattern=r\"(?u)\\b\\w+\\b\",min_df=2).fit_transform(traini['author_ids'].astype('str')).todense())))),axis=1)[traini['overall_worklogs']<1e6],\n",
        "       traini['overall_worklogs'][traini['overall_worklogs']<1e6],test_size=0.1,random_state=0)"
      ],
      "metadata": {
        "id": "_J-0JwOxurJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rgs=catboost.CatBoostRegressor(iterations=2000,\n",
        "                          learning_rate=0.05,loss_function='MAE',\n",
        "                          eval_metric='R2', bagging_temperature=150)"
      ],
      "metadata": {
        "id": "_-QK7DLAu_SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rgs.fit(X_train,y_train,\n",
        "        cat_features=['project_id', \n",
        "       'creator_id', 'dayofweek', 'dayofyear', 'year', 'month', 'day',\n",
        "       'hour', 'assignee_id_active', 'assignee_id_full_name', 'assignee_id_position',\n",
        "       'assignee_id_hiring_type', 'assignee_id_payment_type',\n",
        "       'assignee_id_passport', 'assignee_id_is_nda_signed',\n",
        "       'creator_id_full_name', 'creator_id_position', 'creator_id_hiring_type',\n",
        "       'creator_id_payment_type', 'creator_id_passport',\n",
        "       'creator_id_is_nda_signed', \n",
        "       'creator_id_is_added_to_internal_chats'],\n",
        "        eval_set=(X_val,y_val),verbose=50)"
      ],
      "metadata": {
        "id": "w0EXzc-RvDkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выявление значимых переменных\n",
        "filter=rgs.feature_importances_!=0"
      ],
      "metadata": {
        "id": "16roYVMqvu_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Финальное обучение catboost регрессора"
      ],
      "metadata": {
        "id": "sL1_SIGY1zsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rgs=catboost.CatBoostRegressor(iterations=5000,\n",
        "                          learning_rate=0.05,loss_function='MAE',\n",
        "                          eval_metric='R2', bagging_temperature=150)"
      ],
      "metadata": {
        "id": "paqtxCdCwDR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rgs.fit(pd.concat((traini[['project_id', \n",
        "       'creator_id', 'dayofweek', 'dayofyear', 'year', 'month', 'day',\n",
        "       'hour', 'assignee_id_active', 'assignee_id_full_name', 'assignee_id_position',\n",
        "       'assignee_id_hiring_type', 'assignee_id_payment_type',\n",
        "       'assignee_id_passport', 'assignee_id_is_nda_signed',\n",
        "       'creator_id_full_name', 'creator_id_position', 'creator_id_hiring_type',\n",
        "       'creator_id_payment_type', 'creator_id_passport',\n",
        "       'creator_id_is_nda_signed', 'creator_id_is_added_to_internal_chats', \n",
        "       'ncom', 'sunspots3','days','time', 'time_log']].fillna(-1),\n",
        "       pd.DataFrame(np.hstack((CountVectorizer(min_df=2).fit_transform(traini['summary'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit_transform(traini['summary_l'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 1),min_df=2).fit_transform(traini['summary_tr'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 4),analyzer=\"char\", min_df=2).fit_transform(traini['key'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 4),analyzer=\"char_wb\",min_df=2).fit_transform(traini['assignee_id_full_name'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 4),analyzer=\"char_wb\",min_df=2).fit_transform(traini['creator_id_full_name'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit_transform(traini['texts'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit_transform(traini['texts_l'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 1),min_df=2).fit_transform(traini['texts_tr'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 1),token_pattern=r\"(?u)\\b\\w+\\b\",min_df=2).fit_transform(traini['author_ids'].astype('str')).todense())))),axis=1)[traini['overall_worklogs']<1e6].iloc[:,filter],\n",
        "       traini['overall_worklogs'][traini['overall_worklogs']<1e6],\n",
        "        cat_features=['project_id', \n",
        "       'creator_id', 'dayofweek', 'dayofyear', 'year', 'month', 'day',\n",
        "       'hour', 'assignee_id_active', 'assignee_id_full_name', 'assignee_id_position',\n",
        "       'assignee_id_hiring_type', 'assignee_id_payment_type',\n",
        "       'creator_id_full_name', 'creator_id_position', 'creator_id_hiring_type',\n",
        "       'creator_id_payment_type', 'creator_id_passport'],\n",
        "        sample_weight=(np.cbrt(traini['overall_worklogs'][traini['overall_worklogs']<1e6])/\n",
        "        np.cbrt(traini['overall_worklogs'][traini['overall_worklogs']<1e6]).mean()),verbose=50)"
      ],
      "metadata": {
        "id": "nkT0JIeXv8Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Прогноз\n",
        "testi['overall_worklogs']=rgs.predict(pd.concat((testi[['project_id', \n",
        "       'creator_id', 'dayofweek', 'dayofyear', 'year', 'month', 'day',\n",
        "       'hour', 'assignee_id_active', 'assignee_id_full_name', 'assignee_id_position',\n",
        "       'assignee_id_hiring_type', 'assignee_id_payment_type',\n",
        "       'assignee_id_passport', 'assignee_id_is_nda_signed',\n",
        "       'creator_id_full_name', 'creator_id_position', 'creator_id_hiring_type',\n",
        "       'creator_id_payment_type', 'creator_id_passport',\n",
        "       'creator_id_is_nda_signed', 'creator_id_is_added_to_internal_chats', \n",
        "       'ncom', 'sunspots3','days','time', 'time_log']].fillna(-1),\n",
        "       pd.DataFrame(np.hstack((CountVectorizer(min_df=2).fit(traini['summary'].astype('str')).transform(testi['summary'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit(traini['summary_l'].astype('str')).transform(testi['summary_l'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit(traini['summary_tr'].astype('str')).transform(testi['summary_tr'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 4),analyzer=\"char\", min_df=2).fit(traini['key'].astype('str')).transform(testi['key'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 4),analyzer=\"char_wb\",min_df=2).fit(traini['assignee_id_full_name'].astype('str')).transform(testi['assignee_id_full_name'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 4),analyzer=\"char_wb\",min_df=2).fit(traini['creator_id_full_name'].astype('str')).transform(testi['creator_id_full_name'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit(traini['texts'].astype('str')).transform(testi['texts'].astype('str')).todense(),                    \n",
        "                    CountVectorizer(min_df=2).fit(traini['texts_l'].astype('str')).transform(testi['texts_l'].astype('str')).todense(),\n",
        "                    CountVectorizer(min_df=2).fit(traini['texts_tr'].astype('str')).transform(testi['texts_tr'].astype('str')).todense(),\n",
        "                    CountVectorizer(ngram_range=(1, 1),token_pattern=r\"(?u)\\b\\w+\\b\",min_df=2).fit(traini['author_ids'].astype('str')).transform(testi['author_ids'].astype('str')).todense())))),axis=1).iloc[:,filter])\n",
        "testi['overall_worklogs']=testi['overall_worklogs'].astype('int')\n",
        "submit=testi[['id','overall_worklogs']]\n",
        "submit.to_csv('submit.csv',index=False)"
      ],
      "metadata": {
        "id": "eyLCPDyRwWBc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}